# Fraud Detection MLOps Pipeline

This project demonstrates an end-to-end MLOps workflow for fraud detection using **XGBoost**, **MLflow**, **FastAPI**, **Docker**, and **GitHub Actions**.  
It includes data ingestion, training, experiment tracking, model registry, model promotion, serving, and automated retraining.

---

##  Project Structure

```
fraud_mlops_pipeline/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ app.py                # FastAPI app (serves model from MLflow)
â”‚   â”œâ”€â”€ feature_names.txt     # Saved feature names from training
â”‚
â”‚â”€â”€ training/
â”‚   â”œâ”€â”€ train_model.py        # Model training & logging
â”‚
â”‚â”€â”€ ci/
â”‚   â”œâ”€â”€ promote_if_better.py  # CI script: promote new model if better
â”‚
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ fraud_transactions.csv    # Reduced dataset for testing/CI
â”‚   â”œâ”€â”€ fraud.db                  # SQLite DB created by init_db.py
â”‚   â”œâ”€â”€ generate_monthly_data.py  # Script to simulate 12 month of data for retraining
â”‚
â”‚â”€â”€ mlruns/                   # MLflow experiment & registry store
â”‚â”€â”€ .env.example              # Example environment variables
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ Dockerfile                # Container image for all services
â”‚â”€â”€ docker-compose.yml        # Orchestration of API, training, MLflow UI
â”‚â”€â”€ Makefile                  # Shortcuts (make up, make train, make promote, ...)
â”‚â”€â”€ .github/workflows/mlops.yml # GitHub Actions workflow (automation)
```

---

## ðŸ”‘ Features

- SQLite for persistent fraud data (`data/fraud.db`)
- MLflow for experiment tracking, model logging, registry, and UI
- XGBoost binary classification model
- Automatic promotion if a new model outperforms Production
- FastAPI REST API with API key authentication
- Docker Compose for reproducible services:
  - **mlflow-ui** â†’ experiment tracking dashboard  
  - **api** â†’ model serving  
  - **trainer** â†’ training job  
  - **promote** â†’ promotion job
- GitHub Actions for CI/CD automation:
  - Scheduled retraining (monthly via cron)  
  - Model promotion  
  - Artifacts upload  

---

##  Purpose of `deploy_local_testing` Branch

The `deploy_local_testing` branch is an **experimental extension** of the pipeline that goes beyond local deployment.  
Here, the goal is to **simulate one full year of data drift and monthly retraining**, ensuring that the MLOps workflow can:  

- Handle imbalanced and evolving datasets across time.  
- Persist artifacts and model registry state between GitHub Actions jobs.  
- Automatically restart the API container and validate it with health checks and prediction probes.  
- Resolve schema mismatches by enforcing a consistent feature set (e.g., always including all transaction types).  
- Debug and harden the workflow with retries, artifact cleanup, and fallback strategies.  

This branch demonstrates that the pipeline is **robust against real-world challenges** like schema drift, registry persistence, and CI/CD timing issues.  
It validates that the fraud detection API remains accessible and up to date after every retraining cycle.  

---

##  Using a Self-Hosted Runner

For this branch, a **self-hosted runner** can be configured to automatically deploy the latest Production model locally after retraining.  

### How it works:
1. Set up a self-hosted runner on your local machine (see [GitHubâ€™s guide](https://docs.github.com/en/actions/hosting-your-own-runners/adding-self-hosted-runners)).  
2. Start the runner with `./run.sh` and keep it running.  
3. When the GitHub Actions workflow completes a retraining + promotion cycle, the workflow triggers a **deploy-local** job.  
4. This job downloads the updated `mlruns/` artifact, replaces the local copy, and restarts the FastAPI container.  
5. The API remains continuously reachable over REST with the latest Production model at [http://localhost:8000/docs](http://localhost:8000/docs).  

This setup allows local testing of automation logic while keeping the service always available without cloud infrastructure.  

---

##  Architecture

```mermaid
flowchart LR
    A[SQLite DB: fraud.db] --> B[Training Script train_model.py]
    B -->|Logs metrics, artifacts| C[MLflow Tracking Store mlruns/]
    B -->|Registers model| D[MLflow Model Registry]
    E[Promote Script promote_if_better.py] --> D
    D -->|Production model| F[FastAPI App app.py]
    F -->|REST API /predict| G[Client / Swagger UI]
    H[GitHub Actions Workflow] --> B
    H --> E
```

---

##  Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/<your-username>/fraud_mlops_pipeline.git
   cd fraud_mlops_pipeline
   ```

2. **Create `.env` file (based on `.env.example`)**
   ```env
   API_KEY=...
   PORT=8000
   MLFLOW_TRACKING_URI=file:///workspace/mlruns
   ```

3. **Build Docker images**
   ```bash
   docker compose build
   ```

---

##  Usage

### Start API + MLflow UI
```bash
docker compose up -d mlflow-ui api
```
- **API:** http://127.0.0.1:8000/docs  
- **MLflow:** http://127.0.0.1:5000  

### Train a model
```bash
docker compose run --rm trainer
```
Logs metrics/artifacts to MLflow and registers the model.

### Promote best model
```bash
docker compose run --rm promote
```
Compares new model with current Production and promotes if better.

### Test the API
```bash
curl -X POST "http://127.0.0.1:8000/predict"   -H "Content-Type: application/json"   -H 'x-api-key: your_key'   -d '{"amount": 0.4, "step": 50, "TRANSFER": 1, "PAYMENT": 0, "CASH_OUT": 0, "DEBIT": 0}'
```

**Expected response:**
```json
{"prediction": 1.0}
```

---

##  Automation with GitHub Actions

The project includes a workflow in `.github/workflows/mlops.yml` that automates:

- Running the training pipeline (`train_model.py`)
- Running the promotion logic (`promote_if_better.py`)
- Scheduling retraining automatically once per month (cron job)
- Allowing manual retraining via the **Actions** tab in GitHub
- (Branch-specific) Simulating one year of drifted data, retraining monthly, validating API health
- (Optional) Deploying locally with a self-hosted runner to keep the API updated automatically

This replaces the need for a heavy local Jenkins setup and runs entirely in GitHubâ€™s cloud infrastructure.

---

##  Developer Shortcuts (Makefile)

Instead of typing long Docker commands, use:
```bash
make up        # start API + MLflow UI
make down      # stop everything
make logs      # tail logs
make train     # run training
make promote   # run promotion
```

---

##  Next Steps

- **Monitoring:** Add Prometheus + Grafana to monitor API latency & drift  
- **CI/CD:** Extend GitHub Actions to build and push Docker images to a registry  
- **Cloud:** Deploy containers to AWS ECS, GCP Cloud Run, or Kubernetes  
